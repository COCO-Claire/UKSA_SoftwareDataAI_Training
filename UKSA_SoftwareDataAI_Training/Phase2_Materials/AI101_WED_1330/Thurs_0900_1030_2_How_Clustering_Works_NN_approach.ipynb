{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e371eb1a-0365-4397-801e-01078a855c16",
   "metadata": {
    "id": "e371eb1a-0365-4397-801e-01078a855c16"
   },
   "source": [
    "# How Data Clustering Works\n",
    "\n",
    "In this notebook we will consider a Neural Network (NN) approach to clustering in high dimensional data.\n",
    "\n",
    "Please read through the [How cluster works traditional ML only approach]() notebook before this one as it explains the purpose and basic elements of clustering algorithms.  \n",
    "\n",
    "### What have we learned so far?\n",
    "\n",
    "From the first notebook we learned that a family of methods exists called 'clustering' which divides data into categories which display similar properties in some space. That is, they are near to each other under some definition of distance, that we call a metric.   \n",
    "\n",
    "### Why involve a NN?\n",
    "\n",
    "Now what if we had a set of data with many dimensions and features and our traditional clustering algorithms don't appear to be working that well...\n",
    "\n",
    "This is where exploring some unsupervised NN architectures might be really useful.  In particular algorithms which can reduce the dimensionality of the problem can improve clustering algorithms. Instead of trying to group similar objects in a pre-define metric space we use a NN to learn what the key features are in some lower dimensional space and we then do our grouping there.\n",
    "\n",
    "Before we see an example of this it's worth thinking about dome jargon. You may have come across the term 'embedding' or 'latent space' - what are they?\n",
    "\n",
    "### Embedding and latent spaces\n",
    "\n",
    "In popular literature you'll likely see these terms used interchangeably. However, they do have a small subtle difference.\n",
    "\n",
    "Embedding vectors take high dimensional discrete data and encode it into a lower dimensional vector space. So discrete categorical data is turned into a continuous vector embedding. Now we do use this also with continuous data but where those data are treated like categorical data - this makes sense as we are trying to use an algorithm to cluster, and therefore to place our data into like-minded categories. So our embedding vectors are capturing information about the similarities on the data - sounds similar already to clustering... so an embedding tells us the way the high-dimensional data is mapped to low-dimensional space.\n",
    "\n",
    "A latent space is most often used to mean the space which could contain the embedding vectors but more generally is a space which captures a compressed representation of the features of any data. However, note that occasionally 'latent space' is used for the space of a hidden (latent) variable, which cannot be observed directly.\n",
    "\n",
    "In this notebook we will stick to the terminology embeddings.\n",
    "\n",
    "### The architecture\n",
    "\n",
    "Essentially we want a mechanism which reduces the dimensions. So instead of a network where we have layers of the same size we will have a network where we reduce the layer size with subsequent layers in the network. This will look something like this:\n",
    "\n",
    "![autoencoder_pic](https://github.com/reac2/UKSA_SoftwareDataAI_Training/blob/main/UKSA_SoftwareDataAI_Training/Phase2_Materials/static_images/encoder_image.jpg?raw=true)\n",
    "\n",
    "We will take our inputs, apply a NN where the layers decrease in dimensionality through the layers, and run our clustering algorithm on this learned reduced dimensionality embedding. But how do we actually learn a useful embedding?\n",
    "\n",
    "### What is an Autoencoder?\n",
    "\n",
    "To learn a useful embedding we need another side to our NN - a decoder. The encoder encodes the embedding, reducing the dimensionality and then the decoder take this reduced dimensionality space and expands it back to the original dimensions.\n",
    "\n",
    "![autoencoder_pic](https://github.com/reac2/UKSA_SoftwareDataAI_Training/blob/main/UKSA_SoftwareDataAI_Training/Phase2_Materials/static_images/autoencoder_image.jpg?raw=true)\n",
    "\n",
    "The algorithm will be successful once it has iterated to find an embedding which, for the given embedding size, maximizes the accuracy of the output matching the input. There are therefore no labels needed as the algorithm inputs and 'truth' of the outputs are equal to each other.\n",
    "\n",
    "When the accuracy is high the embedding space must be encoding the 'most useful' information for reconstructing the image, therefore we can see that this space will be a much more efficient one in which to run our clustering algorithm.\n",
    "\n",
    "### So let's give it a go!\n",
    "\n",
    "We'll use a very standard dataset for this example: the MNIST handwritten digits dataset.  \n",
    "\n",
    "This code not mine - must rewrite and want to rewrite to make more understandable."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import Numpy for arrays\n",
    "import numpy as np\n",
    "\n",
    "# Import our PyTorch libraries\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "# We will use the MNIST dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "# Scikit-learn provides metrics to measure the performance of our models.\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score"
   ],
   "metadata": {
    "id": "ymgTb-QWQfYt",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:09:43.478302Z",
     "start_time": "2024-10-01T17:09:31.940783Z"
    }
   },
   "id": "ymgTb-QWQfYt",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are going to use the [MNIST database of handwritten digits](https://yann.lecun.com/exdb/mnist/). We are going to merge the training and test datasets and set up a [Dataloader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders).",
   "id": "28253f076b1fb3e6"
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = MNIST('./', download=True,\n",
    "                 train=True,\n",
    "                 transform=transform)\n",
    "testset = MNIST('./', download=True,\n",
    "                 train=False,\n",
    "                 transform=transform)\n",
    "dataset = ConcatDataset([trainset, testset])\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=256, \n",
    "                        shuffle=True, \n",
    "                        num_workers=8)"
   ],
   "metadata": {
    "id": "0p8_bBsQg_ah",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:11.541312Z",
     "start_time": "2024-10-01T17:10:11.438946Z"
    }
   },
   "id": "0p8_bBsQg_ah",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using {device = }')"
   ],
   "metadata": {
    "id": "ywyM-XC4emIJ",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:12.061043Z",
     "start_time": "2024-10-01T17:10:12.058924Z"
    }
   },
   "id": "ywyM-XC4emIJ",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = device(type='cpu')\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the next two cells, we are going to define our Encoder and Decoder neural networks. We will later combine these into a single neural network as an AutoEncoder.",
   "id": "ee4672f1f7360dfb"
  },
  {
   "cell_type": "code",
   "id": "61c7bf87",
   "metadata": {
    "id": "61c7bf87",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:14.059910Z",
     "start_time": "2024-10-01T17:10:14.053768Z"
    }
   },
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size: int,\n",
    "                 hidden_layers: tuple[int] | list[int],\n",
    "                 dropout_rate: float=0.2,\n",
    "                 activation=nn.ReLU()\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The Encoder network.\n",
    "        A deep neural network that learns a lower-dimensional representation of the input data by mapping it into an embedding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size\n",
    "            Number of features in the input.\n",
    "        hidden_layers\n",
    "            Tuple of number of neurons in the hidden layers.\n",
    "        dropout_rate\n",
    "            Rate at which we will perform dropout in the network.\n",
    "        activation\n",
    "            Specifies the activation function of the network. Defaults to ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # First layer, the input layer\n",
    "        self.input_layer = torch.nn.Linear(input_size, hidden_layers[0])\n",
    "        self.n_layers = 0\n",
    "\n",
    "        ######################################################\n",
    "        # Usually we could specify the layers in this way:\n",
    "        # self.dense_0 = torch.nn.Linear(input_size, hidden_layers[0])\n",
    "        # self.dense_1 = torch.nn.Linear(hidden_layers[0], hidden_layers[1])\n",
    "        # ....\n",
    "        #\n",
    "        # However, instead of hardcoding this, we can do it automatically based on the hidden_layers\n",
    "        # The output of one hidden_layer will always be the input for the next hidden_layer\n",
    "        #######################################################\n",
    "        for i in range(0, len(hidden_layers)-1):\n",
    "            setattr(self, f\"dense_{i}\", torch.nn.Linear(hidden_layers[i],\n",
    "                                                        hidden_layers[i+1])\n",
    "                   )\n",
    "            self.n_layers += 1\n",
    "\n",
    "        self.activation = activation\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "        # Add dropout to prevent overfitting\n",
    "        self.dropout  = nn.Dropout(dropout_rate)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Special Treatment for input layer\n",
    "        x = self.activation(self.input_layer(x))\n",
    "\n",
    "        #################################################\n",
    "        # forward pass through the dense layers\n",
    "        # We could have written each dense layer explicitly:\n",
    "        # x = self.activation(self.dense_0(x))\n",
    "        # x = self.dropout(x)\n",
    "        # x = self.activation(self.dense_1(x))\n",
    "        # .....\n",
    "        #\n",
    "        # But we do it automatically:\n",
    "        ##################################################\n",
    "        for i in range(0, self.n_layers -1):\n",
    "            x = self.activation(getattr(self, f\"dense_{i}\")(x))\n",
    "            # dropout to prevent overfitting\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Use layer without activation function to output embedding\n",
    "        output_layer = getattr(self, f\"dense_{self.n_layers-1}\")\n",
    "        return output_layer(x)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 activation=nn.ReLU()\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Same as the encoder, but the layers are in reverse order.\n",
    "        So, we pass the encoder as input and use its hidden_sizes to specify the decoder network.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        encoder\n",
    "            This is our encoder neural network. We need to pull various attributes from our encoder object to set up our decoder network.\n",
    "        activation\n",
    "            Specifies the activation function of the network. Defaults to ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # We will set our hidden layer sizes to match what we used in the encoder.\n",
    "        self.hidden_layers = encoder.hidden_layers\n",
    "        n_layers = encoder.n_layers\n",
    "        self.hidden_layers = self.hidden_layers[::-1]  # Reverses the order of the hidden layer sizes.\n",
    "\n",
    "        # Reversed order -> dense_0 will be the first to apply here\n",
    "        for i in range(0, n_layers):\n",
    "            setattr(self, f\"dense_{i}\", torch.nn.Linear(self.hidden_layers[i],\n",
    "                                                        self.hidden_layers[i+1])\n",
    "                   )\n",
    "            \n",
    "        # Set the output layer to match the same size as our original input layer.\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_layers[-1],\n",
    "                                                        encoder.input_size)\n",
    "        self.n_layers = n_layers\n",
    "        self.activation = activation\n",
    "        self.dropout  = nn.Dropout(encoder.dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for i in range(0, self.n_layers):\n",
    "            dense_i = getattr(self, f\"dense_{i}\")\n",
    "            x = dense_i(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "        return self.output_layer(x)"
   ],
   "metadata": {
    "id": "ar56y-uee5vo",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:14.667856Z",
     "start_time": "2024-10-01T17:10:14.663472Z"
    }
   },
   "id": "ar56y-uee5vo",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size: int,\n",
    "                 hidden_layers: tuple[int] | list[int],\n",
    "                 dropout_rate: float=0.2,\n",
    "                 activation=nn.ReLU()):\n",
    "        \"\"\"\n",
    "        The complete AutoEncoder that consists of the encoder and the decoder network.\n",
    "        We need this for training, but for applying the autoencoder, we will only need the encoder to map input data to an embedding.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size\n",
    "            Specifies the number of features in the input.\n",
    "        hidden_layers\n",
    "            A tuple of number of neurons in the hidden layers used in the encoder/decoder.\n",
    "        dropout_rate\n",
    "            Rate at which we will perform dropout in the network.\n",
    "        activation\n",
    "            Specifies the activation function of the network. Defaults to ReLU.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_size, hidden_layers, dropout_rate, activation)\n",
    "        self.decoder = Decoder(self.encoder, activation)\n",
    "        self.hidden_layers = hidden_layers\n",
    "\n",
    "    def forward(self, x: Tensor) -> tuple[Tensor, ...]:\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n"
   ],
   "metadata": {
    "id": "nlEi_vuhg3-8",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:15.274551Z",
     "start_time": "2024-10-01T17:10:15.272068Z"
    }
   },
   "id": "nlEi_vuhg3-8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's prepare our input datasets",
   "id": "9c2474c223aad241"
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = trainset.data.numpy().reshape(60000, 784)\n",
    "X_test = testset.data.numpy().reshape(10000, 784)\n",
    "X_test.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJaET_QWg8Gr",
    "outputId": "bac142c9-d3b3-4702-90e9-f035af4f7da6",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:19.385379Z",
     "start_time": "2024-10-01T17:10:19.378917Z"
    }
   },
   "id": "AJaET_QWg8Gr",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "1d4a1e56",
   "metadata": {
    "id": "1d4a1e56",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:19.956471Z",
     "start_time": "2024-10-01T17:10:19.953120Z"
    }
   },
   "source": [
    "y_train = np.array(trainset.targets)\n",
    "y_test = np.array(testset.targets)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "y = np.concatenate([y_train, y_test])\n",
    "X = np.concatenate([X_train, X_test])\n",
    "X.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOX_6MaWhRjr",
    "outputId": "f45270d0-b06b-4f3a-b8ba-e1b57b08a994",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:22.773553Z",
     "start_time": "2024-10-01T17:10:22.759903Z"
    }
   },
   "id": "HOX_6MaWhRjr",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's set up our model and display it.",
   "id": "481115032ff19378"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:28.661223Z",
     "start_time": "2024-10-01T17:10:28.643419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_input_features = X.shape[1]\n",
    "# Initialize architecture of our Auto-Encoder\n",
    "model = AutoEncoder(input_size=n_input_features,\n",
    "                    hidden_layers=[500, 500, 2000,\n",
    "                                 10 # This is the dimension of the embedding\n",
    "                                 ],\n",
    "                   # Prevent overfitting by deactivating 20% of the neurons during training\n",
    "                    dropout_rate=0.2\n",
    "                   ).to(device) # use GPU if available\n",
    "print(model)"
   ],
   "id": "4117511716f8f604",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Encoder(\n",
      "    (input_layer): Linear(in_features=784, out_features=500, bias=True)\n",
      "    (dense_0): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (dense_1): Linear(in_features=500, out_features=2000, bias=True)\n",
      "    (dense_2): Linear(in_features=2000, out_features=10, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dense_0): Linear(in_features=10, out_features=2000, bias=True)\n",
      "    (dense_1): Linear(in_features=2000, out_features=500, bias=True)\n",
      "    (dense_2): Linear(in_features=500, out_features=500, bias=True)\n",
      "    (output_layer): Linear(in_features=500, out_features=784, bias=True)\n",
      "    (activation): ReLU()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's set our training criteria. We will use a loss function that measures the mean-squared error between the input and output of our model as we are train. We will also use stochastic gradient descent with a variable learning rate.",
   "id": "caa4092f73b4359"
  },
  {
   "metadata": {
    "id": "9oiip5QJhTrP",
    "ExecuteTime": {
     "end_time": "2024-10-01T17:10:38.278944Z",
     "start_time": "2024-10-01T17:10:38.275756Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 13,
   "source": [
    "# Set the loss function\n",
    "loss_ = nn.MSELoss()\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.1\n",
    "\n",
    "# Use Stochastic Gradient Descent as optimizer with momentum 0.9\n",
    "optimizer = optim.SGD(lr=lr,\n",
    "                      momentum=0.9,\n",
    "                      params=model.parameters())\n",
    "\n",
    "# Reduce learning rate as training continues\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                      step_size=100,\n",
    "                                      gamma=0.1)"
   ],
   "id": "9oiip5QJhTrP"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The next cell will run the training of the network.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Warning, this may be computationally expensive.\n",
    "\n",
    "</div>"
   ],
   "id": "f980519a0fbb726e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# We could restore a model to continue training from a checkpoint\n",
    "#model = torch.load(\"./torch_models/autoencoder\")\n",
    "\n",
    "n_epochs = 300\n",
    "eval_every = 10\n",
    "best_loss = np.infty\n",
    "\n",
    "# Activate training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    # Iterate over data in batches\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # PyTorch specific; We need to reset all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 0. Transform input batch data from 28 X 28 to 784 features\n",
    "        #   Note that our encoder maps the data into just 10 features!\n",
    "        x_batch = x_batch.to(device)\n",
    "        x_batch = x_batch.view(x_batch.shape[0], -1)\n",
    "\n",
    "        # 1. Apply AutoEncoder model (forward pass).\n",
    "        #    We use the output of the decoder for training.\n",
    "        output = model(x_batch)[1]\n",
    "\n",
    "        # 2. Calculate the reconstruction loss\n",
    "        loss = loss_(output, x_batch)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # 3. Backpropagate less\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    mean_loss = np.round(np.mean(losses), 5)\n",
    "    if (epoch+1) % eval_every == 0:\n",
    "        print(f\"Loss at epoch [{epoch+1} / {n_epochs}]: {mean_loss}\")\n",
    "\n",
    "    # Update learning rate as training continues\n",
    "    scheduler.step()\n",
    "\n",
    "    if mean_loss < best_loss:\n",
    "        best_loss = loss\n",
    "        # Store the model\n",
    "        torch.save(model, \"./torch_models/autoencoder\")"
   ],
   "id": "66456d606d2cf520"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that our model is trained, we can load it up for fine-tuning.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Warning, this may be computationally expensive.\n",
    "\n",
    "</div>"
   ],
   "id": "6ac453d207a7587f"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the model\n",
    "model = torch.load(\"./torch_models/autoencoder\")\n",
    "\n",
    "# Inference Mode for fine-tuning\n",
    "model.eval()\n",
    "\n",
    "lr = 0.1\n",
    "optimizer = torch.optim.SGD(lr=lr,\n",
    "                            momentum=0.9,\n",
    "                            params=model.parameters()\n",
    "                           )\n",
    "n_epochs = 100\n",
    "eval_every = 10\n",
    "best_loss = np.infty\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        # Reset gradients --> Specific for PyTorch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use GPU\n",
    "        x_batch = x_batch.to(device)\n",
    "\n",
    "        # Image has shape 28 x 28 -> Transform to 784 features using flattening\n",
    "        x_batch = x_batch.view(x_batch.shape[0], -1)\n",
    "\n",
    "        # Apply the model\n",
    "        output = model(x_batch)[1]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_(output, x_batch)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = np.round(np.mean(losses), 5)\n",
    "    if (epoch+1) % eval_every == 0:\n",
    "        print(f\"Loss at epoch [{epoch+1} / {n_epochs}]: {mean_loss}\")\n",
    "    torch.save(model, \"./torch_models/autoencoder-finetuned\")"
   ],
   "metadata": {
    "id": "ELu0feHahaKQ",
    "ExecuteTime": {
     "end_time": "2024-10-01T18:42:18.944552Z",
     "start_time": "2024-10-01T17:11:27.655361Z"
    }
   },
   "id": "ELu0feHahaKQ",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch [10 / 100]: 0.05723\n",
      "Loss at epoch [20 / 100]: 0.05422\n",
      "Loss at epoch [30 / 100]: 0.05199\n",
      "Loss at epoch [40 / 100]: 0.05024\n",
      "Loss at epoch [50 / 100]: 0.0488\n",
      "Loss at epoch [60 / 100]: 0.04758\n",
      "Loss at epoch [70 / 100]: 0.04651\n",
      "Loss at epoch [80 / 100]: 0.04559\n",
      "Loss at epoch [90 / 100]: 0.04477\n",
      "Loss at epoch [100 / 100]: 0.04403\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For comparison against a non-neural network method, let's use the [K-Means clustering](https://en.wikipedia.org/wiki/K-means_clustering) from Scikit-learn. ",
   "id": "3b944fc2440e72ba"
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Use the actual number of clusters as parameter\n",
    "n_clusters = len(np.unique(y))\n",
    "\n",
    "# Apply kmeans using sklearn\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=123)\n",
    "\n",
    "# Get training predictions\n",
    "y_pred_kmeans = kmeans.fit_predict(X)"
   ],
   "metadata": {
    "id": "qWefUNiPhdK0",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:18.963975Z",
     "start_time": "2024-10-01T23:34:13.894916Z"
    }
   },
   "id": "qWefUNiPhdK0",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To measure the performance of our clustering methods, we will use the [adjusted mutual information](https://en.wikipedia.org/wiki/Adjusted_mutual_information) (AMI) and [adjusted random index](https://en.wikipedia.org/wiki/Rand_index) (AMI) scores.",
   "id": "2e6a0913f350b1eb"
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Accuracy of k-Means Clustering:\")\n",
    "ami_kmeans = adjusted_mutual_info_score(y, y_pred_kmeans)\n",
    "ari_kmeans = adjusted_rand_score(y, y_pred_kmeans)\n",
    "print(f\"AMI: {ami_kmeans * 100 :.1f}\")\n",
    "print(f\"ARI: {ari_kmeans * 100 :.1f}\")"
   ],
   "metadata": {
    "id": "rADT-f8jhfu1",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:19.063858Z",
     "start_time": "2024-10-01T23:34:18.976187Z"
    }
   },
   "id": "rADT-f8jhfu1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of k-Means Clustering:\n",
      "AMI: 49.1\n",
      "ARI: 36.1\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now, let's load our pre-trained model as it was before we performed our fine-tuning and measure how our predictions compare against the K-Means clustering method.",
   "id": "18142e754f761589"
  },
  {
   "cell_type": "code",
   "source": [
    "model = torch.load(\"./torch_models/autoencoder\")\n",
    "X_embedded_pretrained = model(Tensor(X).to(device))[0]"
   ],
   "metadata": {
    "id": "aHADBky9hi5T",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:27.201721Z",
     "start_time": "2024-10-01T23:34:19.138160Z"
    }
   },
   "id": "aHADBky9hi5T",
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply kmeans using sklearn\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=123)\n",
    "\n",
    "# Convert Data to CPU and apply kmeans to get the cluster predictions\n",
    "y_pred_AE_pretrained = kmeans.fit_predict(X_embedded_pretrained.detach().cpu())"
   ],
   "metadata": {
    "id": "aYxws5MGhkyw",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:27.371541Z",
     "start_time": "2024-10-01T23:34:27.252521Z"
    }
   },
   "id": "aYxws5MGhkyw",
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Accuracy for Auto-Encoder:\")\n",
    "ami_AE_pretrained = adjusted_mutual_info_score(y, y_pred_AE_pretrained)\n",
    "ari_AE_pretrained = adjusted_rand_score(y, y_pred_AE_pretrained)\n",
    "print(f\"AMI: {ami_AE_pretrained * 100:.1f}\")\n",
    "print(f\"ARI: {ari_AE_pretrained * 100:.1f}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "AsvnuMknhn6d",
    "outputId": "f4580270-bd32-4c1d-9665-228d7d1ed066",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:33.930319Z",
     "start_time": "2024-10-01T23:34:33.869768Z"
    }
   },
   "id": "AsvnuMknhn6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Auto-Encoder:\n",
      "AMI: 51.9\n",
      "ARI: 41.6\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, let's load our model after we performed the fine-tuning and compare its performance.",
   "id": "60f3cb377a09298f"
  },
  {
   "cell_type": "code",
   "source": [
    "model = torch.load(\"./torch_models/autoencoder-finetuned\")\n",
    "X_embedded = model(Tensor(X).to(device))[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "r7cVbacNhpoy",
    "outputId": "0ffaa3c8-fc1d-44eb-8bb9-a8bc0a509cfc",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:40.641084Z",
     "start_time": "2024-10-01T23:34:35.543326Z"
    }
   },
   "id": "r7cVbacNhpoy",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply kmeans using sklearn\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=123)\n",
    "\n",
    "# Get training predictions\n",
    "y_pred_AE_finetuned = kmeans.fit_predict(X_embedded.detach().cpu())"
   ],
   "metadata": {
    "id": "3qB-7siBh0xg",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:40.740493Z",
     "start_time": "2024-10-01T23:34:40.670470Z"
    }
   },
   "id": "3qB-7siBh0xg",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Accuracy for Auto-Encoder:\")\n",
    "ami_AE_finetuned = adjusted_mutual_info_score(y, y_pred_AE_finetuned)\n",
    "ari_AE_finetuned = adjusted_rand_score(y, y_pred_AE_finetuned)\n",
    "print(f\"AMI: {ami_AE_finetuned * 100:.1f}\")\n",
    "print(f\"ARI: {ari_AE_finetuned * 100:.1f}\")"
   ],
   "metadata": {
    "id": "MjytLtiGh2mN",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:34:27.663350Z",
     "start_time": "2024-10-01T23:33:54.132893Z"
    }
   },
   "id": "MjytLtiGh2mN",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Auto-Encoder:\n",
      "AMI: 68.7\n",
      "ARI: 62.2\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can use Pandas to put all of our model metric scores together as a convenient summary of the three methods.",
   "id": "d1805dd7d602439a"
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"Clustering Approach\": [\"k-Means\", \"Auto-Encoder (pre-trained)\", \"Auto-Encoder (fine-tuned)\"],\n",
    "                   \"AMI\": [ami_kmeans, ami_AE_pretrained, ami_AE_finetuned],\n",
    "                   \"ARI\": [ari_kmeans, ari_AE_pretrained, ari_AE_finetuned],\n",
    "                   \"AMI Improvement\": [None, (ami_AE_pretrained - ami_kmeans) / ami_AE_pretrained, (ami_AE_finetuned - ami_kmeans) / ami_AE_finetuned],\n",
    "                   \"ARI Improvement\": [None, (ari_AE_pretrained - ari_kmeans) / ari_AE_pretrained, (ari_AE_finetuned - ari_kmeans) / ari_AE_finetuned]})\n",
    "df[\"AMI\"] *= 100\n",
    "df[\"ARI\"] *= 100\n",
    "df[\"AMI Improvement\"] *= 100\n",
    "df[\"ARI Improvement\"] *= 100\n",
    "df[\"AMI\"] = df[\"AMI\"].round(1)\n",
    "df[\"ARI\"] = df[\"ARI\"].round(1)\n",
    "df[\"AMI Improvement\"] = df[\"AMI Improvement\"].round(1)\n",
    "df[\"ARI Improvement\"] = df[\"ARI Improvement\"].round(1)\n",
    "df"
   ],
   "metadata": {
    "id": "09UdjG58h44a",
    "ExecuteTime": {
     "end_time": "2024-10-01T23:41:23.477848Z",
     "start_time": "2024-10-01T23:41:23.469593Z"
    }
   },
   "id": "09UdjG58h44a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          Clustering Approach   AMI   ARI  AMI Improvement  ARI Improvement\n",
       "0                     k-Means  49.1  36.1              NaN              NaN\n",
       "1  Auto-Encoder (pre-trained)  51.9  41.6              5.3             13.2\n",
       "2   Auto-Encoder (fine-tuned)  68.7  62.2             28.5             42.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clustering Approach</th>\n",
       "      <th>AMI</th>\n",
       "      <th>ARI</th>\n",
       "      <th>AMI Improvement</th>\n",
       "      <th>ARI Improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>k-Means</td>\n",
       "      <td>49.1</td>\n",
       "      <td>36.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Auto-Encoder (pre-trained)</td>\n",
       "      <td>51.9</td>\n",
       "      <td>41.6</td>\n",
       "      <td>5.3</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Auto-Encoder (fine-tuned)</td>\n",
       "      <td>68.7</td>\n",
       "      <td>62.2</td>\n",
       "      <td>28.5</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Thus, while the K-Means clustering method is a very fast clustering method, we can improve our accuracy by 5.3% (13.2%) as measured by AMI (ARI) by using a neural network approach. If we fine-tune our initial neural network model we can improve our accuracy by 28.5% (42.0%) when compared to the K-Means cluster method.",
   "id": "1ecbd6fa4b55551f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
